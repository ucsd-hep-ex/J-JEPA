{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e38bf618-b033-4b7c-b679-16ee88faffc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T22:02:53.775298Z",
     "iopub.status.busy": "2024-06-11T22:02:53.774907Z",
     "iopub.status.idle": "2024-06-11T22:02:53.781733Z",
     "shell.execute_reply": "2024-06-11T22:02:53.781098Z",
     "shell.execute_reply.started": "2024-06-11T22:02:53.775273Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import awkward as ak\n",
    "import fastjet\n",
    "import vector\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import argparse\n",
    "import awkward as ak\n",
    "import pandas as pd\n",
    "import mplhep as hep\n",
    "import os\n",
    "import os.path as osp\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5debf1d1-9a16-409f-8249-1f3a45909f69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T21:52:53.355574Z",
     "iopub.status.busy": "2024-06-11T21:52:53.355044Z",
     "iopub.status.idle": "2024-06-11T21:54:43.742871Z",
     "shell.execute_reply": "2024-06-11T21:54:43.742123Z",
     "shell.execute_reply.started": "2024-06-11T21:52:53.355549Z"
    }
   },
   "outputs": [],
   "source": [
    "save_path = \"../data/val\"\n",
    "with h5py.File(f'{save_path}/val_20_30.h5', 'r') as hdf:\n",
    "        # Access the dataset containing your JSON strings\n",
    "    dataset = hdf[\"subjets\"]\n",
    "    \n",
    "    # Initialize a list to hold the deserialized subjets data\n",
    "    subjets_data = []\n",
    "    particles_features = hdf[\"particles\"][\"features\"][:]\n",
    "    particles_labels = hdf[\"particles\"][\"labels\"][:]\n",
    "    \n",
    "    # Iterate over each JSON string in the dataset\n",
    "    for json_str in dataset:\n",
    "        # Deserialize the JSON string to a Python object\n",
    "        subjet = json.loads(json_str)\n",
    "        subjets_data.append(subjet)\n",
    "\n",
    "# At this point, `subjets_data` contains all your subjets as Python dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87122c7a-07c2-44f6-9a59-e83303591219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T21:54:43.744422Z",
     "iopub.status.busy": "2024-06-11T21:54:43.744130Z",
     "iopub.status.idle": "2024-06-11T21:54:43.748672Z",
     "shell.execute_reply": "2024-06-11T21:54:43.748085Z",
     "shell.execute_reply.started": "2024-06-11T21:54:43.744399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(403000, 4, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff375b-79e4-4fb6-8b39-0afdc9d7e98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543cf09-1307-4592-bdce-389e6d228453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86d11c-4cac-427d-9fb1-007dd2faba34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eedb1dd-4494-495e-8825-383f0a3fb119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905a493-46f0-461d-9d14-397565dba852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4a252-0b1d-4275-9f58-a1bc151fc145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78052c91-025b-421c-8df3-8b3a12589c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae521c-1ffb-44d4-9718-afe1b56c9c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e3e4a0-24bf-492f-a222-cdeef7ffc5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f2076-12e6-4f9e-a0bf-c0d30e3fc18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68547f3c-59cd-4b1a-813e-c5e0b9e7ccc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fd278-9516-43b1-9bdf-79c2c73cb990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf08906-2893-44ae-910f-baded31223d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecdabd6e-f669-4f5a-b4ee-fe47c0cf73a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T22:30:09.543792Z",
     "iopub.status.busy": "2024-06-11T22:30:09.543456Z",
     "iopub.status.idle": "2024-06-11T22:32:05.104379Z",
     "shell.execute_reply": "2024-06-11T22:32:05.103491Z",
     "shell.execute_reply.started": "2024-06-11T22:30:09.543768Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = JetDataset(\"../data/val/val_20_30.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c24e5878-430b-43fd-b6d8-9306495ade60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T22:32:18.379115Z",
     "iopub.status.busy": "2024-06-11T22:32:18.378737Z",
     "iopub.status.idle": "2024-06-11T22:33:11.430851Z",
     "shell.execute_reply": "2024-06-11T22:33:11.429968Z",
     "shell.execute_reply.started": "2024-06-11T22:32:18.379092Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 75\u001b[0m\n\u001b[1;32m     71\u001b[0m             total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 67\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m data, targets \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 67\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, targets)\n\u001b[1;32m     69\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[14], line 48\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, src_mask):\n\u001b[0;32m---> 48\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(src, src_mask)\n\u001b[1;32m     50\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe[:x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)]\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_backward_pre_hooks\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import numpy as np\n",
    "# import h5py\n",
    "# import json\n",
    "# import time\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class JetDataset(Dataset):\n",
    "#     def __init__(self, file_path):\n",
    "#         with h5py.File(file_path, 'r') as hdf:\n",
    "#             self.features = torch.tensor(hdf[\"particles/features\"][:], dtype=torch.float32)\n",
    "#             self.labels = torch.tensor(hdf[\"particles/labels\"][:], dtype=torch.long)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.features[idx], self.labels[idx]\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "#         super().__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "#         position = torch.arange(0, max_len).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "#         pe = torch.zeros(max_len, 1, d_model)\n",
    "#         pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.pe[:x.size(0)]\n",
    "#         return self.dropout(x)\n",
    "\n",
    "# class TransformerModel(nn.Module):\n",
    "#     def __init__(self, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "#         super().__init__()\n",
    "#         self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "#         encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "#         self.decoder = nn.Linear(ninp, 2)  # Assuming binary classification\n",
    "\n",
    "#     def forward(self, src, src_mask):\n",
    "#         src = self.pos_encoder(src)\n",
    "#         output = self.transformer_encoder(src, src_mask)\n",
    "#         output = self.decoder(output.mean(dim=1))\n",
    "#         return output\n",
    "\n",
    "# def train():\n",
    "#     # dataset = JetDataset(\"../data/val/val_20_30.h5\")\n",
    "#     dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "#     model = TransformerModel(ninp=128, nhead=2, nhid=200, nlayers=2, dropout=0.5).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "#     for epoch in range(10):\n",
    "#         model.train()\n",
    "#         total_loss = 0.\n",
    "#         for data, targets in dataloader:\n",
    "#             data, targets = data.to(device), targets.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data, None)\n",
    "#             loss = criterion(output, targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "# train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d75611ec-5b55-4570-aae3-8f666054827b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T22:34:35.144506Z",
     "iopub.status.busy": "2024-06-11T22:34:35.144101Z",
     "iopub.status.idle": "2024-06-11T22:51:48.018423Z",
     "shell.execute_reply": "2024-06-11T22:51:48.017766Z",
     "shell.execute_reply.started": "2024-06-11T22:34:35.144482Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████| 12594/12594 [01:40<00:00, 125.14it/s, loss=0.695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.695742585525752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████████| 12594/12594 [01:43<00:00, 121.66it/s, loss=0.683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.6952029269745336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████████| 12594/12594 [01:43<00:00, 122.18it/s, loss=0.694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.6953462037951942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████████| 12594/12594 [01:43<00:00, 121.84it/s, loss=0.692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.6952887247275867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████████████████████████████| 12594/12594 [01:44<00:00, 120.65it/s, loss=0.693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.6952551647568763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████████████████████████████| 12594/12594 [01:43<00:00, 121.38it/s, loss=0.692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.695472660198351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████████████████████████████| 12594/12594 [01:43<00:00, 121.20it/s, loss=0.689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.6954895256908618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████████████████████████████| 12594/12594 [01:43<00:00, 121.93it/s, loss=0.688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.6954986991978116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████████████████████████████| 12594/12594 [01:43<00:00, 121.83it/s, loss=0.738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.6954525886158234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|█████████████████████████████████| 12594/12594 [01:43<00:00, 122.02it/s, loss=0.685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.6952202250574323\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class JetDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        with h5py.File(file_path, 'r') as hdf:\n",
    "            self.features = torch.tensor(hdf[\"particles/features\"][:], dtype=torch.float32)\n",
    "            self.labels = torch.tensor(hdf[\"particles/labels\"][:], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.decoder = nn.Linear(ninp, 2)  # Assuming binary classification\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output.mean(dim=1))\n",
    "        return output\n",
    "\n",
    "def train():\n",
    "    # dataset = JetDataset(\"../data/val/val_20_30.h5\")\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    model = TransformerModel(ninp=128, nhead=2, nhid=200, nlayers=2, dropout=0.5).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0.\n",
    "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}')\n",
    "        for data, targets in progress_bar:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data, None)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss}')\n",
    "\n",
    "    plt.plot(losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51964ab-8370-46e1-bbe5-0558db504d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b8221-af65-4e5b-9005-6acd0ebb6000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532d779-202a-406f-9459-e1bfeb7381f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7188c2-87be-4302-8903-f109f0841f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b52568b-3247-42e3-a3e6-34e475c1c8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27a1d59-216d-4220-8716-9d0ac7a899f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8cc2c-435c-4868-a57e-a507ad171973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f368e7-462d-46b7-ba38-a7b87aad6131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433616a-4867-46f7-99c0-02abe308ce3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1200d59-23da-4773-9065-f215a55e4935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "737b95b2-67c3-4ad3-9184-9ceecd9d831f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T01:06:11.546800Z",
     "iopub.status.busy": "2024-06-12T01:06:11.546402Z",
     "iopub.status.idle": "2024-06-12T01:17:31.356621Z",
     "shell.execute_reply": "2024-06-12T01:17:31.355662Z",
     "shell.execute_reply.started": "2024-06-12T01:06:11.546776Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█████▍                                                | 1/10 [03:17<29:37, 197.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 0.7227946308864305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██████████▊                                           | 2/10 [06:37<26:29, 198.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss: 0.6985211401387437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|████████████████▏                                     | 3/10 [09:52<23:00, 197.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss: 0.6937451358066393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|████████████████▏                                     | 3/10 [11:14<26:13, 224.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m    105\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 75\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     73\u001b[0m features, labels \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     74\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 75\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Assuming output shape is [batch_size, seq_length, num_classes]\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Select the output of the last timestep\u001b[39;00m\n\u001b[1;32m     79\u001b[0m output \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# shape now [batch_size, num_classes]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     61\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mninp)\n\u001b[1;32m     62\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(src)\n\u001b[0;32m---> 63\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/transformer.py:146\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, mask\u001b[38;5;241m=\u001b[39msrc_mask, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask)\n\u001b[0;32m--> 146\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/transformer.py:366\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    360\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(output, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask,\n\u001b[1;32m    361\u001b[0m                  memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    362\u001b[0m                  tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    363\u001b[0m                  memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 366\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2513\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2514\u001b[0m     )\n\u001b[0;32m-> 2515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataset Class\n",
    "class JetDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        with h5py.File(file_path, 'r') as hdf:\n",
    "            self.features = torch.tensor(hdf[\"particles/features\"][:], dtype=torch.float32)\n",
    "            self.labels = torch.tensor(hdf[\"particles/labels\"][:], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        self.encoder = nn.Linear(ntoken, ninp)\n",
    "        self.transformer = nn.Transformer(ninp, nhead, nlayers, nlayers, nhid, dropout)\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "        self.ninp = ninp  # Ensure ninp is correctly set as an instance variable\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer(src, src)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "# Training Function\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features)\n",
    "            \n",
    "            # Assuming output shape is [batch_size, seq_length, num_classes]\n",
    "            # Select the output of the last timestep\n",
    "            output = output[:, -1, :]  # shape now [batch_size, num_classes]\n",
    "            \n",
    "            # Ensure labels are of shape [batch_size]\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "\n",
    "# Load and train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = JetDataset(\"../data/val/val_20_30.h5\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "ntokens = int(dataset.features.size(-1))  # Size of the vocab\n",
    "ninp = 200  # Embedding dimension\n",
    "nhead = 2   # Number of heads in the multiheadattention models\n",
    "nhid = 200  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "dropout = 0.5\n",
    "\n",
    "model = TransformerModel(ntokens, ninp, nhead, nhid, nlayers, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "train(model, dataloader, criterion, optimizer, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3bbeaf-1717-439b-b88d-eff571293ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed01d41-1808-452d-a407-83647234a3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb528b-9463-401b-bba3-8d3aed5f86e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d41181-1f70-45ae-91d1-561cef855d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f628de-b849-4839-978b-2f184ef4d656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f9e259b-2b7f-4d8e-9e0c-81bb49e6ffc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T01:47:14.310879Z",
     "iopub.status.busy": "2024-06-12T01:47:14.310558Z",
     "iopub.status.idle": "2024-06-12T01:48:24.278372Z",
     "shell.execute_reply": "2024-06-12T01:48:24.277491Z",
     "shell.execute_reply.started": "2024-06-12T01:47:14.310856Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = JetDataset(\"../data/val/val_20_30.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acf46b70-2fc4-4a9c-b9e3-271d42bd6a5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T02:39:32.159982Z",
     "iopub.status.busy": "2024-06-12T02:39:32.159595Z",
     "iopub.status.idle": "2024-06-12T02:39:37.596379Z",
     "shell.execute_reply": "2024-06-12T02:39:37.595413Z",
     "shell.execute_reply.started": "2024-06-12T02:39:32.159958Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.90 GiB total capacity; 9.54 GiB already allocated; 21.06 MiB free; 10.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 170\u001b[0m\n\u001b[1;32m    167\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcustom_collate_fn)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# model = IJEPA(embed_dim=384, depth=12, num_heads=6, mlp_ratio=4.0).to(device)\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mIJEPA\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m    172\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.90 GiB total capacity; 9.54 GiB already allocated; 21.06 MiB free; 10.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset Class with Augmentation and Subjet Information\n",
    "class JetDataset(Dataset):\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        with h5py.File(file_path, 'r') as hdf:\n",
    "            self.particles_features = torch.tensor(hdf[\"particles/features\"][:], dtype=torch.float32)\n",
    "            self.subjets = hdf[\"subjets\"][:]\n",
    "        \n",
    "        self.subjets = [json.loads(subjet) for subjet in self.subjets]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subjets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        particles_features, subjets = self.particles_features[idx], self.subjets[idx]\n",
    "        if self.transform:\n",
    "            particles_features = self.transform(particles_features)\n",
    "        return particles_features, subjets\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Vision Transformer Model\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, depth, num_heads, mlp_ratio, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = models.vit_b_16(pretrained=True)\n",
    "        self.encoder.head = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Predictor Network\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, embed_dim, depth, num_heads, mlp_ratio, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = models.vit_b_16(pretrained=True)\n",
    "        self.encoder.head = nn.Identity()\n",
    "\n",
    "    def forward(self, context, mask_tokens):\n",
    "        x = torch.cat([context, mask_tokens], dim=1)\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Joint-Embedding Predictive Architecture (I-JEPA)\n",
    "class IJEPA(nn.Module):\n",
    "    def __init__(self, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.context_encoder = VisionTransformer(embed_dim, depth, num_heads, mlp_ratio)\n",
    "        self.target_encoder = VisionTransformer(embed_dim, depth, num_heads, mlp_ratio)\n",
    "        self.predictor = Predictor(embed_dim, depth, num_heads, mlp_ratio)\n",
    "\n",
    "    def forward(self, context, target, mask_tokens):\n",
    "        context_repr = self.context_encoder(context)\n",
    "        target_repr = self.target_encoder(target)\n",
    "        pred_repr = self.predictor(context_repr, mask_tokens)\n",
    "        return pred_repr, target_repr\n",
    "\n",
    "# Masking function\n",
    "def mask_tokens(features, mask_ratio=0.15):\n",
    "    mask = torch.rand(features.shape) < mask_ratio\n",
    "    mask_tokens = features[mask].clone()\n",
    "    features[mask] = 0\n",
    "    return features, mask_tokens\n",
    "\n",
    "# Preprocess subjets data to convert lists of dictionaries to tensors\n",
    "def preprocess_subjets(subjets):\n",
    "    processed_subjets = []\n",
    "    for subjet in subjets:\n",
    "        processed_subjet = []\n",
    "        for key in subjet:\n",
    "            component = torch.tensor(subjet[key], dtype=torch.float32)\n",
    "            processed_subjet.append(component)\n",
    "        processed_subjets.append(torch.stack(processed_subjet))\n",
    "    \n",
    "    return torch.stack(processed_subjets)\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def custom_collate_fn(batch):\n",
    "    particles_features = torch.stack([item[0] for item in batch])\n",
    "    subjets = [item[1] for item in batch]\n",
    "    return particles_features, subjets\n",
    "\n",
    "# Training Function with Learning Rate Scheduler and Validation\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, num_epochs=10, device='cpu'):\n",
    "    model.train()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        for particles_features, subjets in train_loader:\n",
    "            particles_features = particles_features.to(device)\n",
    "            subjets = preprocess_subjets(subjets).to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Mask tokens\n",
    "            masked_features, mask_tokens = mask_tokens(particles_features)\n",
    "\n",
    "            # Forward pass\n",
    "            pred_repr, target_repr = model(masked_features, subjets, mask_tokens)\n",
    "            loss = criterion(pred_repr, target_repr)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}: Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "        # Validation\n",
    "        validate(model, val_loader, criterion, device)\n",
    "\n",
    "def validate(model, val_loader, criterion, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for particles_features, subjets in val_loader:\n",
    "            particles_features = particles_features.to(device)\n",
    "            subjets = preprocess_subjets(subjets).to(device)\n",
    "            \n",
    "            # Mask tokens\n",
    "            masked_features, mask_tokens = mask_tokens(particles_features)\n",
    "\n",
    "            # Forward pass\n",
    "            pred_repr, target_repr = model(masked_features, subjets, mask_tokens)\n",
    "            loss = criterion(pred_repr, target_repr)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    print(f'Validation Loss: {total_loss / len(val_loader)}')\n",
    "\n",
    "# Load and train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset and split into train and validation\n",
    "# dataset = JetDataset(\"../data/val/val_20_30.h5\")\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# model = IJEPA(embed_dim=384, depth=12, num_heads=6, mlp_ratio=4.0).to(device)\n",
    "model = IJEPA(embed_dim=256, depth=6, num_heads=4, mlp_ratio=4.0).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "train(model, train_loader, val_loader, optimizer, criterion, num_epochs=10, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8b8d264-a558-4b78-869e-a860bc28bc9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T02:43:13.489527Z",
     "iopub.status.busy": "2024-06-12T02:43:13.488898Z",
     "iopub.status.idle": "2024-06-12T02:43:18.925832Z",
     "shell.execute_reply": "2024-06-12T02:43:18.924753Z",
     "shell.execute_reply.started": "2024-06-12T02:43:13.489502Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.90 GiB total capacity; 9.54 GiB already allocated; 21.06 MiB free; 10.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 169\u001b[0m\n\u001b[1;32m    166\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcustom_collate_fn)\n\u001b[1;32m    167\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcustom_collate_fn)\n\u001b[0;32m--> 169\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mIJEPA\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m384\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m    171\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.90 GiB total capacity; 9.54 GiB already allocated; 21.06 MiB free; 10.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "# Dataset Class with Augmentation and Subjet Information\n",
    "class JetDataset(Dataset):\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        with h5py.File(file_path, 'r') as hdf:\n",
    "            self.particles_features = torch.tensor(hdf[\"particles/features\"][:], dtype=torch.float32)\n",
    "            self.subjets = hdf[\"subjets\"][:]\n",
    "        \n",
    "        self.subjets = [json.loads(subjet) for subjet in self.subjets]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subjets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        particles_features, subjets = self.particles_features[idx], self.subjets[idx]\n",
    "        if self.transform:\n",
    "            particles_features = self.transform(particles_features)\n",
    "        return particles_features, subjets\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Vision Transformer Model\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, depth, num_heads, mlp_ratio, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = models.vit_b_16(pretrained=True)\n",
    "        self.encoder.head = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Predictor Network\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, embed_dim, depth, num_heads, mlp_ratio, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = models.vit_b_16(pretrained=True)\n",
    "        self.encoder.head = nn.Identity()\n",
    "\n",
    "    def forward(self, context, mask_tokens):\n",
    "        x = torch.cat([context, mask_tokens], dim=1)\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Joint-Embedding Predictive Architecture (I-JEPA)\n",
    "class IJEPA(nn.Module):\n",
    "    def __init__(self, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.context_encoder = VisionTransformer(embed_dim, depth, num_heads, mlp_ratio)\n",
    "        self.target_encoder = VisionTransformer(embed_dim, depth, num_heads, mlp_ratio)\n",
    "        self.predictor = Predictor(embed_dim, depth, num_heads, mlp_ratio)\n",
    "\n",
    "    def forward(self, context, target, mask_tokens):\n",
    "        context_repr = self.context_encoder(context)\n",
    "        target_repr = self.target_encoder(target)\n",
    "        pred_repr = self.predictor(context_repr, mask_tokens)\n",
    "        return pred_repr, target_repr\n",
    "\n",
    "# Masking function\n",
    "def mask_tokens(features, mask_ratio=0.15):\n",
    "    mask = torch.rand(features.shape) < mask_ratio\n",
    "    mask_tokens = features[mask].clone()\n",
    "    features[mask] = 0\n",
    "    return features, mask_tokens\n",
    "\n",
    "# Preprocess subjets data to convert lists of dictionaries to tensors\n",
    "def preprocess_subjets(subjets):\n",
    "    processed_subjets = []\n",
    "    for subjet in subjets:\n",
    "        processed_subjet = [torch.tensor(subjet[key], dtype=torch.float32) for key in subjet]\n",
    "        processed_subjets.append(torch.stack(processed_subjet))\n",
    "    \n",
    "    return torch.stack(processed_subjets)\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def custom_collate_fn(batch):\n",
    "    particles_features = torch.stack([item[0] for item in batch])\n",
    "    subjets = [item[1] for item in batch]\n",
    "    return particles_features, subjets\n",
    "\n",
    "# Training Function with Learning Rate Scheduler and Validation\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, num_epochs=10, device='cpu'):\n",
    "    model.train()\n",
    "    scaler = amp.GradScaler()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        for particles_features, subjets in train_loader:\n",
    "            particles_features = particles_features.to(device)\n",
    "            subjets = preprocess_subjets(subjets).to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Mask tokens\n",
    "            masked_features, mask_tokens = mask_tokens(particles_features)\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            with amp.autocast():\n",
    "                pred_repr, target_repr = model(masked_features, subjets, mask_tokens)\n",
    "                loss = criterion(pred_repr, target_repr)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}: Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "        # Validation\n",
    "        validate(model, val_loader, criterion, device)\n",
    "\n",
    "def validate(model, val_loader, criterion, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for particles_features, subjets in val_loader:\n",
    "            particles_features = particles_features.to(device)\n",
    "            subjets = preprocess_subjets(subjets).to(device)\n",
    "            \n",
    "            # Mask tokens\n",
    "            masked_features, mask_tokens = mask_tokens(particles_features)\n",
    "\n",
    "            # Forward pass\n",
    "            with amp.autocast():\n",
    "                pred_repr, target_repr = model(masked_features, subjets, mask_tokens)\n",
    "                loss = criterion(pred_repr, target_repr)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    print(f'Validation Loss: {total_loss / len(val_loader)}')\n",
    "\n",
    "# Load and train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset and split into train and validation\n",
    "# dataset = JetDataset(\"../data/val/val_20_30.h5\")\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "model = IJEPA(embed_dim=384, depth=12, num_heads=6, mlp_ratio=4.0).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "train(model, train_loader, val_loader, optimizer, criterion, num_epochs=10, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b59acb-05bd-4182-8b1d-51d828c0791e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
